{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f82438",
   "metadata": {},
   "source": [
    "# Neural Network \n",
    "\n",
    "This project focuses on creating a Neural Network from scratch to gain a better understanding of fundamental concepts like feed forward, gradient descent, and backpropagation. The MNIST dataset is used for testing and validation.\n",
    "\n",
    "* ## [1. Introduction](#1-Introduction)\n",
    "* ## [2. Model Architecture](#2-Model-Architecture)\n",
    "    * ### [2.1 Forward Pass](#21-Forward-Pass)\n",
    "    * ### [2.2 Loss Function](#22-Loss-Function)\n",
    "    * ### [2.3 Backpropagation](#23-Backpropagation)\n",
    "    * ### [2.4 Gradient Descent](#24-Gradient-Descent)\n",
    "* ## [3. Code Implementation](#3-Code-Implementation)\n",
    "* ## [4. Model Training Results](#4-Model-Training-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85cf46c",
   "metadata": {},
   "source": [
    "## <a name=\"1-Introduction\">1. Introduction</a> \n",
    "\n",
    "In this project, we explore the construction of a simple feedforward Neural Network with customizable layers and neuron counts. Our primary objectives are to investigate and implement fundamental concepts, namely feedforward, gradient descent, and backpropagation. While this neural network constructor may be basic, it serves as an effective tool for comprehending these crucial topics in depth.\n",
    "\n",
    "In addition to learning how to create a basic neural network, we will test it using the MNIST dataset to recognize handwritten digits and observe how our simple Neural Network performs with this well-known example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbe067b",
   "metadata": {},
   "source": [
    "## <a name=\"2-Model-Architecture\">2. Model Architecture</a> \n",
    "\n",
    "\n",
    "A Feedforward Neural Network consists of layers composed of interconnected neurons, connected through weights. In this type of neural network, information can only move forward, without forming cycles or undergoing backward steps.\n",
    "\n",
    "<img src=\"img/feedforward.png\" alt=\"feedforward\" width=\"85%\">\n",
    "\n",
    "\n",
    "Each neuron has weights connected to it from the previous layer (except for the input) and another set connected to the next layer (except for the output). The nomenclature used is as follows:\n",
    "\n",
    "- <sub style=\"font-size: 1.3em\">$a_i^l$</sub>:  Activation of the $i$-th neuron in layer $𝑙$\n",
    "\n",
    "\n",
    "- <sub style=\"font-size: 1.3em\">$z_i^l$</sub>:  Pre-activation of the $i$-th neuron in layer $𝑙$ (before goes into activation function)\n",
    "\n",
    "\n",
    "- <sub style=\"font-size: 1.3em\">$w_{ij}^l$</sub>: Weight from $i$-th neuron in layer $(l-1)$ to $j$-th neuron in layer $l$\n",
    "\n",
    "\n",
    "- <sub style=\"font-size: 1.3em\">$b_i^l$</sub>:  Bias for neuron $i$ in layer $l$\n",
    "\n",
    "\n",
    "The input layer is considered as $a^{(0)}$ and the output layer as $a^{(n)}$, where $n$ is the number of layers mimnus one.\n",
    "\n",
    "The activation ($a$) and pre-activation ($z$) are given by: \n",
    "$$ z_j^{(l)} = \\sum_{i=1}^{n_{l-1}} w_{ij}^{(l)} a_i^{(l-1)} + b_j^{(l)} $$\n",
    "\n",
    "$$ a_j^{(l)} = f^{(l)}(z_j^{(l)}) $$\n",
    "\n",
    "Where $n_{l-1}$ is the number of neurons in layer $𝑙 − 1$, and $f^{(l)}$ is the activation function for layer $l$.\n",
    "\n",
    "---\n",
    "\n",
    "### <a name=\"21-Forward-Pass\">2.1 Forward Pass</a> \n",
    "\n",
    "The forward pass involves calculations performed throughout the neural network, from the input to the output. To perform the calculation, you need to compute the activation for each neuron and continue this process until you reach the output. For a specific layer, the activations are computed as follows:\n",
    "\n",
    "$$a^{(l)} = f^{(l)}(  (𝑊^{(l)})^Ta^{(l-1)}+b^{(l)}  )$$\n",
    "\n",
    "Where $W$ is the matrix for the weights of layer $l$, $a^{(l-1)}$ the vector of activations from layer $(l-1)$ and $b^{(l)}$ the vector of bias from layer $l$. The weight matrix, activation vector and bias vector are represented this way:\n",
    "\n",
    "\n",
    "\n",
    "$$𝑊^{(l)} = \n",
    "\\begin{bmatrix}\n",
    "    w_{11}^{(𝑙)} & \\cdots & w_{1n_𝑙}^{(𝑙)} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n_{𝑙-1}1}^{(𝑙)} & \\cdots & w_{n_{𝑙-1}n_𝑙}^{(𝑙)} \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$a^{(𝑙)} =\n",
    "\\begin{bmatrix}\n",
    "    a_1^{(𝑙)} \\\\\n",
    "    \\vdots \\\\\n",
    "    a_{n_𝑙}^{(𝑙)}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$b^{(𝑙)} =\n",
    "\\begin{bmatrix}\n",
    "    b_1^{(𝑙)} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n_𝑙}^{(𝑙)}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The matrix $𝑊(𝑙)$ of dimension $𝑛_{𝑙−1} × 𝑛_𝑙$ contains the weights of the layer $𝑙$.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <a name=\"22-Loss-Function\">2.2 Loss Function</a> \n",
    "\n",
    "\n",
    "The Loss Function quantifies the difference between the predicted output and the actual target. The goal during training is to minimize this discrepancy. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification problems. \n",
    "The loss function help us to see how far we are from our target, and its based on its value that we try to minimize.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <a name=\"23-Backpropagation\">2.3 Backpropagation</a> \n",
    "\n",
    "Backpropagation is the fundamental algorithm that enables neural networks to learn. It involves propagating the output error backward through the network and adjusting the weights and biases based on their respective contributions to the output. The key is to minimize the error between the output and the target.\n",
    "\n",
    "The level of adjustment is determined by the gradients of the cost function concerning those parameters (weights or biases). Gradients are utilized due to their capability to measure the sensitivity of the output function concerning its variables.\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial x}$$\n",
    "\n",
    "* The derivative tells us the direction C is going.\n",
    "* The gradient shows how much the parameter x needs to change (in positive or negative direction) to minimize C.\n",
    "\n",
    "To facilitate error propagation throughout the network, we use the Chain Rule for derivatives. This method is particularly crucial in deep neural networks. An easy demostration of the chain rule is:\n",
    "\n",
    "if $y=f(u)$, where $u=g(x)$, you can write:\n",
    "\n",
    "$$\\frac{d y}{d x} = \\frac{d y}{d u}  \\frac{d u}{d x}$$\n",
    "\n",
    "To have a better understanding on this topic you can check this [page](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction).\n",
    "With the help of this chain rule we can use the backpropagation to calculate the gradient for each weight and bias in the entire network:\n",
    "\n",
    "$$weights: \\frac{d C}{d w_{11}^{(1)}}, \\frac{d C}{d w_{12}^{(1)}}, ... , \\frac{d C}{d w_{i(j-1)}^{(l)}}, \\frac{d C}{d w_{ij}^{(l)}}$$\n",
    "\n",
    "\n",
    "$$bias: \\frac{d C}{d b_1^{(1)}}, \\frac{d C}{d b_2^{(1)}}, ... , \\frac{d C}{d b_{(j-1)}^{(l)}}, \\frac{d C}{d b_{j}^{(l)}}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### <a name=\"24-Gradient-Descent\">2.4 Gradient Descent</a> \n",
    "\n",
    "Gradient Descent is an optimization algorithm that is used to help train machine learning models. It makes use of the gradients computed by the backpropagation to update the value of weights and bias, always tending to minimize the loss function. This algorithm is used repetively in the trainning process to move the variables to a local minimal in the superficie of the loss function.\n",
    "\n",
    "<img src=\"img/gradient-descent.png\" alt=\"gradient-descent\" width=\"70%\">\n",
    "\n",
    "Even though the primary aim is to minimize the loss function significantly, achieving the global minimum isn't assured; the algorithm typically converges towards a local minimum, which might or might not be the global minimum. Techniques such as weight and bias initialization methods, along with optimization features like ADAM and momentum, can be employed to alleviate this issue.\n",
    "Let's explore how the Gradient Descent algorithm utilizes gradients. To update a specific element, you follow this formula:\n",
    "\n",
    "$$ x = x - \\alpha \\cdot \\nabla L_x $$\n",
    "\n",
    "- $x$ represents the specific weight or bias being updated.\n",
    "- $\\alpha$ denotes the learning rate, controlling the size of optimization steps.\n",
    "- $\\nabla L_x$ signifies the gradient of the loss function with respect to the weights or biases, indicating the direction and rate of the steepest change in the loss function.\n",
    "\n",
    "The learning rate ($\\alpha$) determines the size of steps taken to reach the minimum. Typically a small value, it's assessed and adjusted based on the cost function's behavior. Higher learning rates lead to larger steps, risking overshooting the minimum. Conversely, lower learning rates yield smaller step sizes.\n",
    "\n",
    "<img src=\"img/learning_rate.png\" alt=\"learning_rate\" width=\"70%\">\n",
    "\n",
    "You can also divide the way you update the elements using gradient descent in 3 main groups:\n",
    "\n",
    "* Batch Gradient Descent\n",
    "\n",
    "In this approach, the gradients for all points in the training dataset are summed up, and updates are made only after computing all of them. This method usually results in a smoother curve towards local minima but may demand significant time and memory due to the need to store all gradients before updating the values.\n",
    "\n",
    "* Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates the values after processing each input in the dataset. This technique is faster and requires less memory as updates occur after every forward pass. However, it can introduce considerable noise since each input modifies the elements related to its output, ignoring other samples.\n",
    "\n",
    "* Mini-Batch Gradient Descent\n",
    "\n",
    "This technique attempts to combine the advantages of the above methods. The training dataset is divided into smaller mini-datasets, and updates to weights and biases occur after processing each mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c6901",
   "metadata": {},
   "source": [
    "## <a name=\"3-Code-Implementation\">3. Code Implementation</a> \n",
    "\n",
    "I've designed a class called NeuralNetwork, requiring a list containing integers as input. Each element in the list represents a layer in the neural network, including both the input and output layers. The integer at each position signifies the number of neurons in that layer. Here's an example for clarification:\n",
    "\n",
    "```python\n",
    "nn = NeuralNetwork(layers=[4,3,2])\n",
    "```\n",
    "<img src=\"nn-example.png\" alt=\"nn-example\" width=\"70%\">\n",
    "\n",
    "All weight and bias matrices are instances of this class, created during initialization. Both weight and bias matrices are generated using a random normal distribution with an average of 0 and a standard deviation of 1. \n",
    "You can also select the learning rate for the gradient descent update, but I've made it optional. The default value is $0.5$. The entire initialization function includes instances of activations, gradients, and the number of layers, built as shown in the following code snippet:\n",
    "\n",
    "```python\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, layers, learning_rate=.5):\n",
    "        self.n_layers = len(layers)\n",
    "        # Weights\n",
    "        self.weights = [np.random.normal(loc=0.0, scale=1.0,\n",
    "                        size=(layers[i], layers[i+1])) for i in range(len(layers)-1)]\n",
    "        # Bias\n",
    "        self.bias = [np.random.normal(loc=0.0, scale=1.0,\n",
    "                     size=(i,)) for i in layers[1:]]\n",
    "        # Learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        # Activations \n",
    "        self.a = []\n",
    "        # Gradients\n",
    "        self.grad_w = []\n",
    "        self.grad_b = []\n",
    "```\n",
    "\n",
    "The forward pass only require the input data since all weights and biases are instantiated within the class. Additionally, for this project, I've employed the sigmoid activation function for all layers in the neural network. The code was implemented as follows:\n",
    "\n",
    "$$z^{(l)} = (𝑊^{(l)})^Ta^{(l-1)}+b^{(l)}$$\n",
    "\n",
    "$$ a^{(l)} = \\sigma(z^{(l)}) $$\n",
    "\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def forward_pass(self, a0):\n",
    "        a = a0\n",
    "        self.a.append(a)\n",
    "\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w.transpose(), a) + b\n",
    "            a = sigmoid(z)\n",
    "            self.a.append(a)\n",
    "        return a\n",
    "```\n",
    "\n",
    "\n",
    "After performing the forward pass, the next step involves computing the error of the loss function (L2, chosen for this project). Using this value, we calculate the gradients for each weight and bias. In this project, considering all activation functions as sigmoids and the L2 loss function, the gradient computation for the last layer can be achieved as follows:\n",
    "\n",
    "* Calculating the loss ($l$ is the output layer):\n",
    "\n",
    "\n",
    "$$L(y, a^{(l)}) = \\frac {1}{2} (y - a^{(l)})^2 $$\n",
    "\n",
    "\n",
    "* The gradient for $w^{(l)}$ and $b^{(l)}$ using the Chain Rule of derivatives are: \n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l)}} = \\frac {dz^{(l)}}{dw^{(l)}} \\frac {da^{(l)}}{dz^{(l)}} \\frac {dL(y, a^{(l)})}{da^{(l)}}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l)}} = \\frac {dz^{(l)}}{db^{(l)}} \\frac {da^{(l)}}{dz^{(l)}} \\frac {dL(y, a^{(l)})}{da^{(l)}}$$\n",
    "\n",
    "\n",
    "* Calculating each of this derivatives separately, we have:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{da^{(l)}} = a^{(l)} - y$$\n",
    "\n",
    "    \n",
    "$$\\frac {da^{(l)}}{dz^{(l)}} =  \\frac {d\\sigma(z^{(l)})}{dz^{(l)}} = \\sigma(z^{(l)}) (1 - \\sigma(z^{(l)}))$$\n",
    "\n",
    "\n",
    "$$\\frac {dz^{(l)}}{dw^{(l)}} = a^{(l-1)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dz^{(l)}}{db^{(l)}} = 1$$\n",
    "\n",
    "\n",
    "* Multiplying the derivatives:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l)}} = a^{(l-1)} \\cdot \\sigma(z^{(l)}) (1 - \\sigma(z^{(l)})) \\cdot (a^{(l)} - y)$$\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l)}} = 1 \\cdot \\sigma(z^{(l)}) (1 - \\sigma(z^{(l)})) \\cdot (a^{(l)} - y)$$\n",
    "\n",
    "\n",
    "* Lets name $\\sigma(z^{(l)}) (1 - \\sigma(z^{(l)})) \\cdot (a^{(l)} - y)$ as $\\delta^{(l)}$, so we are left with:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l)}} = a^{(l-1)} \\cdot \\delta^{(l)} \\quad(I)$$\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l)}} = 1 \\cdot\\delta^{(l)} \\quad(I)$$\n",
    "\n",
    "\n",
    "* We've computed the gradients for the weight and bias matrices of the output layer. Now, let's attempt to create a formula to update the weights and biases for all the hidden layers. First, let's calculate the gradients for the layer preceding the output one:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-1)}} = \\frac {dz^{(l-1)}}{dw^{(l-1)}} \\frac {da^{(l-1)}}{dz^{(l-1)}} \\frac {dz^{(l)}}{da^{(l-1)}} \\frac {da^{(l)}}{dz^{(l)}} \\frac {dL(y, a^{(l)})}{da^{(l)}}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-1)}} = \\frac {dz^{(l-1)}}{db^{(l-1)}} \\frac {da^{(l-1)}}{dz^{(l-1)}} \\frac {dz^{(l)}}{da^{(l-1)}} \\frac {da^{(l)}}{dz^{(l)}} \\frac {dL(y, a^{(l)})}{da^{(l)}}$$\n",
    "\n",
    "\n",
    "* We know that for the output layer, $ \\delta^{(l)} = \\sigma(z^{(l)}) (1 - \\sigma(z^{(l)})) \\cdot (a^{(l)} - y)$, so let's substitute it into the equation:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-1)}} = \\frac {dz^{(l-1)}}{dw^{(l-1)}} \\frac {da^{(l-1)}}{dz^{(l-1)}} \\frac {dz^{(l)}}{da^{(l-1)}} \\delta^{(l)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-1)}} = \\frac {dz^{(l-1)}}{db^{(l-1)}} \\frac {da^{(l-1)}}{dz^{(l-1)}} \\frac {dz^{(l)}}{da^{(l-1)}} \\delta^{(l)}$$\n",
    "\n",
    "\n",
    "* Calculating each of this derivatives separately, we have:\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac {da^{(l-1)}}{dz^{(l-1)}} = \\frac {d\\sigma(z^{(l-1)})}{dz^{(l-1)}} = \\sigma(z^{(l-1)}) (1 - \\sigma(z^{(l-1)}))$$\n",
    "\n",
    "\n",
    "$$\\frac {dz^{(l)}}{da^{(l-1)}} = w^{(l)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dz^{(l-1)}}{dw^{(l-1)}} = a^{(l-2)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dz^{(l-1)}}{db^{(l-1)}} = 1$$\n",
    "\n",
    "\n",
    "* Multiplying the derivatives:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-1)}} = a^{(l-2)} \\cdot \\sigma(z^{(l-1)}) (1 - \\sigma(z^{(l-1)})) \\cdot w^{(l)} \\cdot \\delta^{(l)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-1)}} = 1 \\cdot \\sigma(z^{(l-1)}) (1 - \\sigma(z^{(l-1)})) \\cdot w^{(l)} \\cdot \\delta^{(l)}$$\n",
    "\n",
    "\n",
    "* Lets name $\\sigma(z^{(l-1)}) (1 - \\sigma(z^{(l-1)})) \\cdot w^{(l)} \\cdot \\delta^{(l)}$ as $\\delta^{(l-1)}$, so we are left with:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-1)}} = a^{(l-2)} \\cdot \\delta^{(l-1)} \\quad(II)$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-1)}} = 1 \\cdot \\delta^{(l-1)} \\quad(II)$$\n",
    "\n",
    "\n",
    "* You can begin to observe a pattern between $(I)$ and $(II)$, where the gradients of the weights and bias directly depend on this $\\delta$ term. We simply need to formulate a method to obtain $\\delta$ for each layer in the neural network, enabling us to compute the gradients. So, let's attempt to do that by first creating a relationship between $\\delta^{(l)}$ and $\\delta^{(l-1)}$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta^{(l-1)} = \\delta^{(l)} \\cdot w^{(l)} \\cdot \\sigma'(z^{(l-1)}) \\quad (*)\n",
    "$$\n",
    "\n",
    "\n",
    "* To ensure the applicability of this formula to subsequent hidden layers, let's proceed to compute the gradients for $w^{(l-2)}$ and $b^{(l-2)}$:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-2)}} = \\frac {dz^{(l-2)}}{dw^{(l-2)}} \\frac {da^{(l-2)}}{dz^{(l-2)}} \\frac {dz^{(l-1)}}{da^{(l-2)}} \\frac {da^{(l-1)}}{dz^{(l-1)}} \\frac {dz^{(l)}}{da^{(l-1)}} \\frac {da^{(l)}}{dz^{(l)}} \\frac {dL(y, a^{(l)})}{da^{(l)}}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-2)}} = \\frac {dz^{(l-2)}}{db^{(l-2)}} \\frac {da^{(l-2)}}{dz^{(l-2)}} \\frac {dz^{(l-1)}}{da^{(l-2)}} \\frac {da^{(l-1)}}{dz^{(l-1)}} \\frac {dz^{(l)}}{da^{(l-1)}} \\frac {da^{(l)}}{dz^{(l)}} \\frac {dL(y, a^{(l)})}{da^{(l)}}$$\n",
    "\n",
    "\n",
    "* Substituting $\\delta^{(l)}$, $\\delta^{(l-1)}$:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-2)}} = \\frac {dz^{(l-2)}}{dw^{(l-2)}} \\frac {da^{(l-2)}}{dz^{(l-2)}} \\frac {dz^{(l-1)}}{da^{(l-2)}} \\delta^{(l-1)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-2)}} = \\frac {dz^{(l-2)}}{db^{(l-2)}} \\frac {da^{(l-2)}}{dz^{(l-2)}} \\frac {dz^{(l-1)}}{da^{(l-2)}} \\delta^{(l-1)}$$\n",
    "\n",
    "\n",
    "* Calculating the derivatives separately and multiplying them, we are left with:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-2)}} = a^{(l-3)} \\cdot \\sigma(z^{(l-2)}) (1 - \\sigma(z^{(l-2)})) \\cdot w^{(l-1)} \\cdot \\delta^{(l-1)}$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-2)}} = 1 \\cdot \\sigma(z^{(l-2)}) (1 - \\sigma(z^{(l-2)})) \\cdot w^{(l-1)} \\cdot \\delta^{(l-1)}$$\n",
    "\n",
    "\n",
    "* Naming name $\\sigma(z^{(l-2)}) (1 - \\sigma(z^{(l-2)})) \\cdot w^{(l-1)} \\cdot \\delta^{(l-1)}$ as $\\delta^{(l-2)}$, we have:\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{dw^{(l-2)}} = a^{(l-3)} \\cdot \\delta^{(l-2)} \\quad(III)$$\n",
    "\n",
    "\n",
    "$$\\frac {dL(y, a^{(l)})}{db^{(l-2)}} = 1 \\cdot \\delta^{(l-2)} \\quad(III)$$\n",
    "\n",
    "\n",
    "* Creating a relationship between $\\delta^{(l-2)}$ and $\\delta^{(l-1)}$:\n",
    "\n",
    "\n",
    "$$ \\delta^{(l-2)} = \\sigma(z^{(l-2)}) (1 - \\sigma(z^{(l-2)})) \\cdot w^{(l-1)} \\cdot \\delta^{(l-1)} $$\n",
    "\n",
    "\n",
    "$$ \\delta^{(l-2)} = \\delta^{(l-1)} \\cdot w^{(l-1)} \\cdot \\sigma'(z^{(l-2)}) $$\n",
    "\n",
    "\n",
    "* Wich is exactly the formula we got on $(*)$:\n",
    "\n",
    "\n",
    "$$\\delta^{(l-1)} = \\delta^{(l)} \\cdot w^{(l)} \\cdot \\sigma'(z^{(l-1)}) \\quad (*)$$\n",
    "\n",
    "\n",
    "* Lastly, we need to address matrix multiplications. We utilize matrix multiplications because they simplify the computation of gradients for each weight and bias and enhance the code and notation for better comprehension. For updating the weights and bias matrices, it's important that the gradient matrix for each element matches their shape. This leads us to perform the matrix multiplications in the following manner:\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = (\\delta^{(l+1)} \\cdot W^{(l+1)}) \\odot \\sigma'(Z^{(l)})\n",
    "$$\n",
    "\n",
    "\n",
    "* By doing this multiplication it will result in a matrix of shape $(n,1)$, where $n$ is the number of neuron in layer $l$. Let's see how the elements of the $\\delta$ vector will look like:\n",
    "\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} =\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(𝑙+1)} \\\\\n",
    "\\vdots \\\\\n",
    "\\delta_n^{(𝑙+1)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_{11}^{(𝑙+1)} & \\cdots & w_{1n}^{(𝑙+1)} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m1}^{(𝑙+1)} & \\cdots & w_{mn}^{(𝑙+1)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\sigma'(Z^{(l)})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} =\n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(𝑙+1)}w_{11}^{(𝑙+1)} + \\delta_2^{(𝑙+1)}w_{12}^{(𝑙+1)} + \\cdots + \\delta_{n}^{(𝑙+1)}w_{1n}^{(𝑙+1)} \\\\\n",
    "\\vdots \\\\\n",
    "\\delta_1^{(𝑙+1)}w_{m1}^{(𝑙+1)} + \\delta_2^{(𝑙+1)}w_{m2}^{(𝑙+1)} + \\cdots + \\delta_{n}^{(𝑙+1)}w_{mn}^{(𝑙+1)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\sigma'(Z^{(l)})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma'(z_1^{(l)})(\\delta_1^{(𝑙+1)}w_{11}^{(𝑙+1)} + \\delta_2^{(𝑙+1)}w_{12}^{(𝑙+1)} + \\cdots + \\delta_{n}^{(𝑙+1)}w_{1n}^{(𝑙+1)}) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma'(z_m^{(l)})(\\delta_1^{(𝑙+1)}w_{m1}^{(𝑙+1)} + \\delta_2^{(𝑙+1)}w_{m2}^{(𝑙+1)} + \\cdots + \\delta_{n}^{(𝑙+1)}w_{mn}^{(𝑙+1)}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* Now that we have $\\delta^{(l)}$, we can calculate the gradients for the weights and biases of layer $l$, as demonstrated by equations $(I)$, $(II)$, and $(III)$. Let's proceed with the calculations:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla W^{(l)} = (\\delta^{(l)})^T \\cdot a^{(l-1)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla b^{(l)} = \\delta^{(l)} \\cdot 1\n",
    "$$\n",
    "\n",
    "\n",
    "* The gradient for the bias is straightforward. Let's proceed with the gradient for the weight:\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla W^{(l)} = (\\delta^{(l)})^T \\cdot a^{(l-1)}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\nabla W^{(l)} = \n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(𝑙)} \\cdots \\delta_m^{(𝑙)}\n",
    "\\end{bmatrix} \n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "a_1^{(𝑙-1)} \\\\\n",
    "\\vdots \\\\\n",
    "a_k^{(𝑙-1)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\nabla W^{(l)} = \n",
    "\\begin{bmatrix}\n",
    "\\delta_1^{(𝑙)}  a_1^{(𝑙-1)} & \\delta_1^{(𝑙)}  a_2^{(𝑙-1)} &  \\cdots & \\delta_1^{(𝑙)}  a_k^{(𝑙-1)} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\delta_m^{(𝑙)}  a_1^{(𝑙-1)} & \\delta_m^{(𝑙)}  a_2^{(𝑙-1)} & \\cdots & \\delta_m^{(𝑙)}  a_k^{(𝑙-1)} \\end{bmatrix}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Notice how the $\\delta$ term primarily stores derivatives from previous layers, simplifying the computation of derivative chain rules. The formula I employed in this project functions in the following manner:\n",
    "\n",
    "\n",
    "* For the output layer:\n",
    "\n",
    "$$\\delta^{(n)} = A^{(n)} (1 - A^{(n)}) \\cdot (A^{(n)} - Y)$$\n",
    "\n",
    "* For hidden layers:\n",
    "\n",
    "$$\n",
    "\\delta^{(l)} = (\\delta^{(l+1)} \\cdot (W^{(l+1)})) \\cdot f'(Z^{(l)})\n",
    "$$\n",
    "\n",
    "* Weights gradient:\n",
    "\n",
    "$$\n",
    "∇W^{(l)} = A^{(l-1)} \\cdot (\\delta^{(l)})^{T}\n",
    "$$\n",
    "\n",
    "* Bias gradient:\n",
    "\n",
    "$$\n",
    "∇b^{(l)} = \\delta^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88578cb4",
   "metadata": {},
   "source": [
    "##### Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48b8bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec31e3",
   "metadata": {},
   "source": [
    "##### Loading MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fed6a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aad309",
   "metadata": {},
   "source": [
    "##### Activation funtion & Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c9ab883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def d_sigmoid(a):\n",
    "    return a * (1 - a)    \n",
    "\n",
    "def l2(y_pred, y_targ):\n",
    "    return .5 * (y_targ - y_pred)**2\n",
    "\n",
    "def d_l2(y_pred, y_targ):\n",
    "    return y_pred - y_targ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ca20b",
   "metadata": {},
   "source": [
    "###### Neural Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3b3daf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers, learning_rate=.5):\n",
    "        self.n_layers = len(layers)\n",
    "        # Weights\n",
    "        self.weights = [np.random.normal(loc=0.0, scale=1.0,\n",
    "                        size=(layers[i], layers[i+1])) for i in range(len(layers)-1)]\n",
    "        # Bias\n",
    "        self.bias = [np.random.normal(loc=0.0, scale=1.0,\n",
    "                        size=(i,)) for i in layers[1:]]\n",
    "        # Learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        # Activations \n",
    "        self.a = []\n",
    "        # Gradients\n",
    "        self.grad_w = [0] * (self.n_layers-1)\n",
    "        self.grad_b = [0] * (self.n_layers-1)\n",
    "        \n",
    "    def reset_gradients(self):\n",
    "        self.grad_w = [0] * (self.n_layers-1)\n",
    "        self.grad_b = [0] * (self.n_layers-1)\n",
    "        \n",
    "    def reset_activations(self):\n",
    "        self.a = []\n",
    "        \n",
    "    def forward_pass(self, a0):\n",
    "        a = a0\n",
    "        self.a.append(a)\n",
    "\n",
    "        for w, b in zip(self.weights, self.bias):\n",
    "            z = np.dot(w.transpose(), a) + b # 𝑧(𝑙)=(𝑊(𝑙))𝑇𝑎(𝑙−1)+𝑏(𝑙)\n",
    "            a = sigmoid(z)                   # 𝑎(𝑙)=𝜎(𝑧(𝑙))\n",
    "            self.a.append(a)\n",
    "        return a\n",
    "    \n",
    "    def delta(self, y_targ):\n",
    "        deltas = []\n",
    "        \n",
    "        for n in range(self.n_layers-1, 0, -1):\n",
    "            # output layer\n",
    "            if n == self.n_layers-1: \n",
    "                delta = d_sigmoid(self.a[n]) * d_l2(self.a[n], y_targ)  # 𝛿(𝑛)=𝐴(𝑛)(1−𝐴(𝑛))⋅(𝐴(𝑛)−𝑌)\n",
    "                deltas.append(delta)\n",
    "                \n",
    "            # hidden layer\n",
    "            else:\n",
    "                delta = np.dot(self.weights[n], delta) * d_sigmoid(self.a[n]) # 𝛿(𝑙)=(𝛿(𝑙+1)⋅(𝑊(𝑙+1)))⋅𝑓′(𝑍(𝑙))\n",
    "                deltas.append(delta)\n",
    "                \n",
    "        deltas = deltas[::-1] # invert the delta list so that it is in the same order as (0, 1, 2, ...)\n",
    "        \n",
    "        return deltas\n",
    "        \n",
    "    \n",
    "    def backpropagation(self, y_targ):        \n",
    "        deltas = self.delta(y_targ)\n",
    "        \n",
    "        grad_w = []\n",
    "        grad_b = []\n",
    "        \n",
    "        for n in range(self.n_layers-1):\n",
    "            a = self.a[n].reshape(-1, 1)\n",
    "            d = deltas[n].reshape(-1, 1)\n",
    "            \n",
    "            g_w = np.dot(a, d.transpose()) # ∇𝑊(𝑙)=𝐴(𝑙−1)⋅(𝛿(𝑙))𝑇\n",
    "            g_b = d.reshape(-1)            # ∇𝑏(𝑙)=𝛿(𝑙)\n",
    "            \n",
    "            self.grad_w[n] += g_w\n",
    "            self.grad_b[n] += g_b\n",
    "            \n",
    "            \n",
    "    def gradient_descent(self, batch_size=1):\n",
    "        self.grad_w = [g_w / batch_size for g_w in self.grad_w]\n",
    "        self.grad_b = [g_b / batch_size for g_b in self.grad_b]\n",
    "\n",
    "        for n in range(self.n_layers-1):\n",
    "            # updanting Weights\n",
    "            self.weights[n] -= self.learning_rate * self.grad_w[n]\n",
    "        \n",
    "            # updating Bias\n",
    "            self.bias[n] -= self.learning_rate * self.grad_b[n]\n",
    "            \n",
    "    def train(self, x_train, y_train, epochs=25, batch_size=1):\n",
    "        num_samples = x_train.shape[0]\n",
    "        epoch_loss = []\n",
    "        batch_loss = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            acumulated_loss = 0\n",
    "            # shuffle the training data for each epoch\n",
    "            x_train_shuffled, y_train_shuffled = shuffle(x_train, y_train)\n",
    "\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                # Get the current batch\n",
    "                x_batch = x_train_shuffled[i:i + batch_size]\n",
    "                y_batch = y_train_shuffled[i:i + batch_size]\n",
    "                \n",
    "                for n in range(batch_size):\n",
    "                    x = x_batch[n].ravel()\n",
    "                    y = np.zeros(10)\n",
    "                    y[y_batch[n]] = 1\n",
    "\n",
    "                    # Forward pass\n",
    "                    output = self.forward_pass(a0=x)\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = l2(y_pred=output, y_targ=y)\n",
    "                    acumulated_loss += sum(loss)/len(loss)\n",
    "\n",
    "                    # Backpropagation\n",
    "                    self.backpropagation(y_targ=y)\n",
    "                    \n",
    "                    # Reset activations\n",
    "                    nn.reset_activations()\n",
    "                \n",
    "                # Batch loss\n",
    "                batch_loss.append(acumulated_loss/batch_size)\n",
    "\n",
    "                # Gradient descent\n",
    "                self.gradient_descent(batch_size=batch_size)\n",
    "                \n",
    "                # Reset gradients\n",
    "                nn.reset_gradients()\n",
    "                \n",
    "            # Epoch loss\n",
    "            train_loss = acumulated_loss/num_samples\n",
    "            epoch_loss.append(train_loss)\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs} - Loss: {train_loss:.4f}\")\n",
    "            \n",
    "        return epoch_loss, batch_loss\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28745950",
   "metadata": {},
   "source": [
    "## <a name=\"4-Model-Training-Results\">4. Model Training Results</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4e43a48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35 - Loss: 0.0317\n",
      "Epoch 2/35 - Loss: 0.0174\n",
      "Epoch 3/35 - Loss: 0.0133\n",
      "Epoch 4/35 - Loss: 0.0112\n",
      "Epoch 5/35 - Loss: 0.0099\n",
      "Epoch 6/35 - Loss: 0.0091\n",
      "Epoch 7/35 - Loss: 0.0085\n",
      "Epoch 8/35 - Loss: 0.0080\n",
      "Epoch 9/35 - Loss: 0.0076\n",
      "Epoch 10/35 - Loss: 0.0074\n",
      "Epoch 11/35 - Loss: 0.0071\n",
      "Epoch 12/35 - Loss: 0.0069\n",
      "Epoch 13/35 - Loss: 0.0067\n",
      "Epoch 14/35 - Loss: 0.0066\n",
      "Epoch 15/35 - Loss: 0.0065\n",
      "Epoch 16/35 - Loss: 0.0063\n",
      "Epoch 17/35 - Loss: 0.0062\n",
      "Epoch 18/35 - Loss: 0.0061\n",
      "Epoch 19/35 - Loss: 0.0060\n",
      "Epoch 20/35 - Loss: 0.0059\n",
      "Epoch 21/35 - Loss: 0.0059\n",
      "Epoch 22/35 - Loss: 0.0058\n",
      "Epoch 23/35 - Loss: 0.0057\n",
      "Epoch 24/35 - Loss: 0.0057\n",
      "Epoch 25/35 - Loss: 0.0056\n",
      "Epoch 26/35 - Loss: 0.0055\n",
      "Epoch 27/35 - Loss: 0.0055\n",
      "Epoch 28/35 - Loss: 0.0054\n",
      "Epoch 29/35 - Loss: 0.0054\n",
      "Epoch 30/35 - Loss: 0.0053\n",
      "Epoch 31/35 - Loss: 0.0053\n",
      "Epoch 32/35 - Loss: 0.0052\n",
      "Epoch 33/35 - Loss: 0.0052\n",
      "Epoch 34/35 - Loss: 0.0051\n",
      "Epoch 35/35 - Loss: 0.0051\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([784, 16, 10])\n",
    "epoch_loss, batch_loss = nn.train(x_train, y_train, epochs=35, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "748db22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs=35\n",
    "\n",
    "plt.plot(range(epochs), epoch_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Plot of loss Over epoch')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "61e0d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = 925; f = 75\n"
     ]
    }
   ],
   "source": [
    "f = 0\n",
    "v = 0\n",
    "for n in range(1000):\n",
    "    x = x_test[n].ravel()\n",
    "    y = y_test[n]\n",
    "    y_pred = nn.forward_pass(x)\n",
    "    nn.reset_activations()\n",
    "    if np.argmax(y_pred) == y:\n",
    "        v +=1\n",
    "    else:\n",
    "        f+=1\n",
    "print(f\"v = {v}; f = {f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
